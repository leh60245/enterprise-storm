#!/usr/bin/env python
"""
Enterprise STORM Pipeline - ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ì¼ê´„ ìƒì„±

PostgreSQL ë‚´ë¶€ DBë¥¼ í™œìš©í•œ ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
ì™¸ë¶€ ê²€ìƒ‰ ì—”ì§„ ëŒ€ì‹  PostgresRMì„ ì‚¬ìš©í•˜ì—¬ DART ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

í†µí•© ì•„í‚¤í…ì²˜:
    - src.common.config: í†µí•© ì„¤ì • (DB, AI, Embedding)
    - src.common.embedding: í†µí•© ì„ë² ë”© ì„œë¹„ìŠ¤ (ì°¨ì› ê²€ì¦ í¬í•¨)
    - knowledge_storm: STORM ì—”ì§„ (PostgresRM ì‚¬ìš©)

Required Environment Variables:
    - OPENAI_API_KEY: OpenAI API key
    - GOOGLE_API_KEY: Google Gemini API key (--model-provider gemini ì‚¬ìš© ì‹œ)
    - EMBEDDING_PROVIDER: 'huggingface' ë˜ëŠ” 'openai' (DBì™€ ì¼ì¹˜ í•„ìˆ˜!)
    - PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE: PostgreSQL ì ‘ì† ì •ë³´

âš ï¸ ì¤‘ìš”: EMBEDDING_PROVIDERëŠ” DBì— ì €ì¥ëœ ë²¡í„° ì°¨ì›ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤!
    - HuggingFace: 768ì°¨ì›
    - OpenAI: 1536ì°¨ì›

Output Structure:
    results/
        topic_name/
            conversation_log.json
            raw_search_results.json
            storm_gen_outline.txt
            url_to_info.json
            storm_gen_article.txt
            storm_gen_article_polished.txt

Usage:
    python -m scripts.run_storm --topic "ì‚¼ì„±ì „ì SWOT ë¶„ì„"
    python -m scripts.run_storm --batch  # ë°°ì¹˜ ëª¨ë“œ (ANALYSIS_TARGETS ì‚¬ìš©)

Author: Enterprise STORM Team
Updated: 2026-01-11 - Unified Architecture with Dimension Validation
"""

import os
import sys
import re
import json
import logging
from datetime import datetime
from argparse import ArgumentParser

import psycopg2
from psycopg2.extras import Json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.common.config import TOPICS, DB_CONFIG
from src.common.db_utils import get_available_companies

from knowledge_storm import (
    STORMWikiRunnerArguments,
    STORMWikiRunner,
    STORMWikiLMConfigs,
)
from knowledge_storm.lm import OpenAIModel, AzureOpenAIModel, GoogleModel
from knowledge_storm.rm import PostgresRM, SerperRM, HybridRM
from knowledge_storm.utils import load_api_key

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def select_company_and_topic() -> tuple[int, str, str]:
    """
    CLI ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ: ê¸°ì—… ë° ì£¼ì œ ì„ íƒ

    DBì—ì„œ ê¸°ì—… ëª©ë¡ì„ ì¡°íšŒí•˜ì—¬ ë²ˆí˜¸ ë©”ë‰´ë¡œ ì¶œë ¥í•˜ê³ ,
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ê¸°ì—…ëª…ê³¼ ë¶„ì„ ì£¼ì œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        tuple[int, str, str]: (ê¸°ì—…ID, ê¸°ì—…ëª…, ë¶„ì„ ì£¼ì œ)

    Raises:
        SystemExit: DBì—ì„œ ê¸°ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
    """
    # 1. ê¸°ì—… ì„ íƒ
    companies = get_available_companies()
    if not companies:
        print("âŒ [Error] DBì—ì„œ ì¡°íšŒëœ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. DB ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    print("\n" + "=" * 50)
    print("        [ Enterprise STORM ë¶„ì„ê¸° ]")
    print("=" * 50)
    print("\nğŸ¢ ë¶„ì„í•  ê¸°ì—…ì„ ì„ íƒí•˜ì„¸ìš”:")

    for company_id, company_name in companies:
        print(f"  [{company_id}] {company_name}")

    target_company = (0, "")
    while True:
        try:
            sel = input("\nğŸ‘‰ ê¸°ì—… ë²ˆí˜¸ ì…ë ¥: ").strip()
            company_id = int(sel)
            if any(cid == company_id for cid, _ in companies):
                target_company = next((cid, name) for cid, name in companies if cid == company_id)
                break
            else:
                print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # 2. ì£¼ì œ ì„ íƒ
    topics = list()
    for topic in TOPICS:
        topics.append(topic["label"])
        
    print(f"\nğŸ“ [{target_company[1]}] ê´€ë ¨ ë¶„ì„ ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    for idx, topic in enumerate(topics):
        print(f"  [{idx + 1}] {topic}")

    target_topic = ""
    while True:
        try:
            sel = input("\nğŸ‘‰ ì£¼ì œ ë²ˆí˜¸ ì…ë ¥: ").strip()
            idx = int(sel) - 1
            if 0 <= idx < len(topics):
                if idx == len(topics) - 1:  # ììœ  ì£¼ì œ
                    target_topic = input("   âœï¸  ì§ˆë¬¸í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                    if not target_topic:
                        print("âš ï¸ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        continue
                else:
                    target_topic = topics[idx]
                break
            else:
                print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    print(f"\nâœ… ë¶„ì„ ì‹œì‘: {target_company[1]} - {target_topic}")
    return target_company[0], target_company[1], target_topic





def _safe_dir_component(name: str, fallback: str = "unknown") -> str:
    """ë””ë ‰í† ë¦¬ ê²½ë¡œ ì»´í¬ë„ŒíŠ¸ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤ (Windows ê¸ˆì§€ë¬¸ì ì œê±°, ê³µë°±->ì–¸ë”ìŠ¤ì½”ì–´)."""
    if not name:
        return fallback
    safe = name.replace(" ", "_")
    safe = safe.replace("/", "_").replace("\\", "_")
    safe = re.sub(r'[:*?"<>|]', "", safe)
    safe = safe.strip(". ")
    return safe or fallback


def build_run_output_dir(base_output_dir: str, company_id: int, company_name: str = None) -> str:
    """
    ì‹¤í–‰ë³„ ê²°ê³¼ í´ë”ë¥¼ `base/YYYYMMDD_HHMMSS_company_id/` í˜•íƒœë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    
    Flat structureë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ + company_idë¡œ ê³ ìœ ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ ê²½ë¡œ ê¸¸ì´ ì œí•œ ë¬¸ì œë¥¼ íšŒí”¼í•˜ê³  ë””ë²„ê¹…ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.
    
    Args:
        base_output_dir: ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬
        company_id: ê¸°ì—… ID (ê³ ìœ ì„± ë³´ì¥ìš©)
        company_name: ê¸°ì—…ëª… (ë””ë ‰í† ë¦¬ ëª…ì— í¬í•¨í•  ìˆ˜ ìˆìŒ, ì„ íƒì‚¬í•­)
    
    Returns:
        ìƒì„±ëœ ê²°ê³¼ í´ë” ê²½ë¡œ
    """
    # íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ ë‹¨ìœ„)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # company_nameì´ ìˆìœ¼ë©´ ì•ˆì „í•˜ê²Œ ë³€í™˜í•˜ì—¬ ì ‘ë¯¸ì‚¬ë¡œ ì¶”ê°€
    if company_name:
        company_suffix = _safe_dir_component(company_name, fallback="company")
        dir_name = f"{timestamp}_{company_id}_{company_suffix}"
    else:
        dir_name = f"{timestamp}_{company_id}"
    
    run_dir = os.path.join(base_output_dir, dir_name)
    
    # ê°™ì€ ì´ˆì— ì¬ì‹¤í–‰/ë³‘ë ¬ ì‹¤í–‰ ì‹œ ì¶©ëŒ ë°©ì§€
    suffix = 1
    candidate = run_dir
    while os.path.exists(candidate):
        suffix += 1
        candidate = f"{run_dir}_{suffix}"
    
    os.makedirs(candidate, exist_ok=True)
    return candidate


def write_run_args_json(run_output_dir: str, *, topic: str, company_id: int, company_name: str, args, model_name: str):
    """ì‹¤í–‰ í´ë”ì— ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ ì„¤ì •ì„ JSONìœ¼ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤."""
    payload = {
        "created_at": datetime.now().isoformat(timespec="seconds"),
        "topic": topic,
        "company_id": company_id,
        "company_name": company_name,
        "model_provider": getattr(args, "model_provider", None),
        "model_name": model_name,
        "output_dir": run_output_dir,
        "storm_args": {
            "max_conv_turn": getattr(args, "max_conv_turn", None),
            "max_perspective": getattr(args, "max_perspective", None),
            "search_top_k": getattr(args, "search_top_k", None),
            "min_score": getattr(args, "min_score", None),
            "max_thread_num": getattr(args, "max_thread_num", None),
            "do_research": getattr(args, "do_research", None),
            "do_generate_outline": getattr(args, "do_generate_outline", None),
            "do_generate_article": getattr(args, "do_generate_article", None),
            "do_polish_article": getattr(args, "do_polish_article", None),
        },
        "env": {
            "OPENAI_API_TYPE": os.getenv("OPENAI_API_TYPE"),
            "EMBEDDING_PROVIDER": os.getenv("EMBEDDING_PROVIDER"),
            "PG_HOST": os.getenv("PG_HOST"),
            "PG_PORT": os.getenv("PG_PORT"),
            "PG_DATABASE": os.getenv("PG_DATABASE"),
        },
    }

    path = os.path.join(run_output_dir, "run_args.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)


def save_report_to_db(ai_query: str, output_dir: str, secrets_path: str, model_name: str, company_id: int, company_name: str, analysis_topic: str) -> bool:
    """
    âœ… [REFACTOR] Uses centralized DBManager.insert_generated_report()
    
    STORM ì‹¤í–‰ ê²°ê³¼ë¥¼ PostgreSQLì˜ Generated_Reports í…Œì´ë¸”ì— ì ì¬í•©ë‹ˆë‹¤.
    
    í´ë” êµ¬ì¡°:
        base/YYYYMMDD_HHMMSS_company_id_company_name/
            {ai_query}/  â† STORM runnerê°€ ìƒì„±í•˜ëŠ” í´ë”
                conversation_log.json
                storm_gen_outline.txt
                storm_gen_article_polished.txt
                url_to_info.json
                raw_search_results.json
                ...

    Args:
        ai_query: LLMì—ê²Œ ì…ë ¥ëœ ì‹¤ì œ ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸ (í´ë”ëª…ìœ¼ë¡œë„ ì‚¬ìš©ë¨)
        output_dir: STORM ì‹¤í–‰ ê²°ê³¼ ê¸°ë³¸ ë””ë ‰í† ë¦¬ (= run_output_dir)
        secrets_path: ë¹„ë°€ ì •ë³´ íŒŒì¼ ê²½ë¡œ
        model_name: ì‚¬ìš©í•œ ëª¨ë¸ëª… ('openai' ë˜ëŠ” 'gemini')
        company_id: Companies tableì˜ ID (í•„ìˆ˜, FK)
        company_name: ê¸°ì—…ëª…
        analysis_topic: ë¶„ì„ ì£¼ì œ (DBì— ì €ì¥í•  topic í•„ë“œ)

    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    
    # ========================================
    # Step 1: íŒŒì¼ ê²½ë¡œ êµ¬ì„±
    # ========================================
    # STORM runnerëŠ” {ai_query}ë¥¼ íŒŒì¼ì‹œìŠ¤í…œ ì•ˆì „ ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ íŒŒì¼ ìƒì„±
    # ê³µë°± â†’ ì–¸ë”ë°”(_), ê¸ˆì§€ë¬¸ì ì œê±°
    safe_topic_dir = _safe_dir_component(ai_query)
    topic_output_dir = os.path.join(output_dir, safe_topic_dir)
    
    logger.info(f"Reading STORM output from: {topic_output_dir}")
    
    # ========================================
    # Step 2: í•„ìˆ˜ íŒŒì¼ ì½ê¸°
    # ========================================
    # storm_gen_article_polished.txt (í•„ìˆ˜)
    polished_article_path = os.path.join(topic_output_dir, "storm_gen_article_polished.txt")
    if not os.path.exists(polished_article_path):
        logger.error(f"Required file not found: {polished_article_path}")
        return False

    with open(polished_article_path, "r", encoding="utf-8") as f:
        report_content = f.read()

    # url_to_info.json (í•„ìˆ˜)
    url_to_info_path = os.path.join(topic_output_dir, "url_to_info.json")
    if not os.path.exists(url_to_info_path):
        logger.error(f"Required file not found: {url_to_info_path}")
        return False

    with open(url_to_info_path, "r", encoding="utf-8") as f:
        references_data = json.load(f)

    # ========================================
    # Step 2: ì„ íƒ íŒŒì¼ ì½ê¸°
    # ========================================
    # storm_gen_outline.txt (ì„ íƒ)
    toc_text = None
    outline_path = os.path.join(topic_output_dir, "storm_gen_outline.txt")
    if os.path.exists(outline_path):
        with open(outline_path, "r", encoding="utf-8") as f:
            toc_text = f.read()

    # conversation_log.json (ì„ íƒ)
    conversation_log = None
    conv_log_path = os.path.join(topic_output_dir, "conversation_log.json")
    if os.path.exists(conv_log_path):
        with open(conv_log_path, "r", encoding="utf-8") as f:
            conversation_log = json.load(f)

    # run_config.json (ì„ íƒ)
    run_config_data = None
    config_path = os.path.join(topic_output_dir, "run_config.json")
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            run_config_data = json.load(f)

    # raw_search_results.json (ì„ íƒ)
    raw_search_results_data = None
    search_results_path = os.path.join(topic_output_dir, "raw_search_results.json")
    if os.path.exists(search_results_path):
        with open(search_results_path, "r", encoding="utf-8") as f:
            raw_search_results_data = json.load(f)

    # ========================================
    # Step 3: meta_info ìƒì„±
    # ========================================
    meta_info = {
        "config": run_config_data,
        "search_results": raw_search_results_data
    }

    # ========================================
    # Step 4: DBì— ì €ì¥ (âœ… REFACTOR: Use DBManager)
    # ========================================
    try:
        from src.ingestion.db_manager import DBManager
        
        with DBManager() as db:
            report_id = db.insert_generated_report(
                company_name=company_name,
                topic=analysis_topic,
                report_content=report_content,
                toc_text=toc_text,
                references_data=references_data or {},
                conversation_log=conversation_log or {},
                meta_info=meta_info or {},
                model_name=model_name,
                company_id=company_id
            )
        
        if report_id:
            logger.info(f"âœ“ Report saved to DB: {analysis_topic} (report_id={report_id}, company_id={company_id}, company_name={company_name})")
            return True
        else:
            logger.error(f"âœ— Failed to save report to DB (no ID returned)")
            return False

    except Exception as e:
        logger.error(f"âœ— Failed to save report to DB: {e}")
        return False


def setup_lm_configs(provider: str = "openai") -> STORMWikiLMConfigs:
    """
    LLM ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    Args:
        provider: LLM ê³µê¸‰ì ('openai' ë˜ëŠ” 'gemini')

    Returns:
        STORMWikiLMConfigs: ì„¤ì •ëœ LM êµ¬ì„± ê°ì²´
    """
    lm_configs = STORMWikiLMConfigs()

    if provider == "gemini":
        # Google Gemini ëª¨ë¸ ì„¤ì •
        gemini_kwargs = {
            "temperature": 1.0,
            "top_p": 0.9,
        }

        # Gemini ëª¨ë¸ëª… ì„¤ì • (2026ë…„ ìµœì‹  í˜•ì‹: models/ ì ‘ë‘ì‚¬ ì—†ì´ ì‚¬ìš©)
        gemini_flash_model = "gemini-2.0-flash"
        gemini_pro_model = "gemini-2.0-flash"

        
        conv_simulator_lm = GoogleModel(
            model=gemini_flash_model, max_tokens=2048, **gemini_kwargs  # í† í° ìˆ˜ ì•½ê°„ ìƒí–¥
        )
        question_asker_lm = GoogleModel(
            model=gemini_flash_model, max_tokens=2048, **gemini_kwargs
        )
        outline_gen_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=4096, **gemini_kwargs
        )
        article_gen_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=8192, **gemini_kwargs
        )
        article_polish_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=8192, **gemini_kwargs
        )

        logger.info(f"âœ“ Using Gemini models: {gemini_flash_model} (fast), {gemini_pro_model} (pro)")

    else:
        # OpenAI ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’)
        openai_kwargs = {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "temperature": 1.0,
            "top_p": 0.9,
        }

        # API íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ
        api_type = os.getenv("OPENAI_API_TYPE", "openai")
        ModelClass = OpenAIModel if api_type == "openai" else AzureOpenAIModel

        # ëª¨ë¸ëª… ì„¤ì •
        gpt_large_model = "gpt-4o-mini" 
        gpt_fast_model = "gpt-4o"

        # Azure ì„¤ì • (í•„ìš”ì‹œ)
        if api_type == "azure":
            openai_kwargs["api_base"] = os.getenv("AZURE_API_BASE")
            openai_kwargs["api_version"] = os.getenv("AZURE_API_VERSION")

        conv_simulator_lm = ModelClass(
            model=gpt_large_model, max_tokens=500, **openai_kwargs
        )
        question_asker_lm = ModelClass(
            model=gpt_large_model, max_tokens=500, **openai_kwargs
        )
        outline_gen_lm = ModelClass(
            model=gpt_fast_model, max_tokens=400, **openai_kwargs
        )
        article_gen_lm = ModelClass(
            model=gpt_fast_model, max_tokens=700, **openai_kwargs
        )
        article_polish_lm = ModelClass(
            model=gpt_fast_model, max_tokens=4000, **openai_kwargs
        )

        logger.info(f"âœ“ Using OpenAI models: {gpt_large_model} (fast), {gpt_fast_model} (pro)")

    # ê° ì»´í¬ë„ŒíŠ¸ë³„ LM ì„¤ì •
    # - conv_simulator_lm, question_asker_lm: ë¹ ë¥¸ ëª¨ë¸ (ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜)
    # - outline_gen_lm, article_gen_lm, article_polish_lm: ê°•ë ¥í•œ ëª¨ë¸ (ì½˜í…ì¸  ìƒì„±)
    lm_configs.set_conv_simulator_lm(conv_simulator_lm)
    lm_configs.set_question_asker_lm(question_asker_lm)
    lm_configs.set_outline_gen_lm(outline_gen_lm)
    lm_configs.set_article_gen_lm(article_gen_lm)
    lm_configs.set_article_polish_lm(article_polish_lm)

    return lm_configs


def fix_topic_json_encoding(ai_query: str, output_dir: str):
    """
    ìƒì„±ëœ ê²°ê³¼ í´ë” ë‚´ JSON íŒŒì¼ë“¤ì˜ ì¸ì½”ë”©ì„ ë³´ì •í•©ë‹ˆë‹¤.
    STORMì´ ìƒì„±í•œ ai_query ê¸°ë°˜ í•˜ìœ„ í´ë” ë‚´ì˜ JSON íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        ai_query: LLMì—ê²Œ ì…ë ¥ëœ ì§ˆë¬¸ (STORMì´ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©)
        output_dir: STORM ì‹¤í–‰ ê²°ê³¼ ê¸°ë³¸ ë””ë ‰í† ë¦¬ (= run_output_dir)
    """
    # STORMì´ ìƒì„±í•œ ì‹¤ì œ í´ë” ê²½ë¡œ êµ¬ì„± (ê³µë°±â†’ì–¸ë”ë°” ë³€í™˜)
    safe_topic_dir = _safe_dir_component(ai_query)
    topic_output_dir = os.path.join(output_dir, safe_topic_dir)
    
    if not os.path.exists(topic_output_dir):
        logger.warning(f"Output directory not found for encoding fix: {topic_output_dir}")
        return

    logger.info(f"Fixing JSON encoding in: {topic_output_dir}")

    # topic_output_dir ë‚´ì˜ JSON íŒŒì¼ë§Œ ìˆœíšŒí•˜ì—¬ ì¸ì½”ë”© ë³´ì •
    try:
        for file in os.listdir(topic_output_dir):
            if file.endswith(".json"):
                file_path = os.path.join(topic_output_dir, file)
                try:
                    # ì½ê¸°
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)

                    # ë‹¤ì‹œ ì“°ê¸° (ensure_ascii=False)
                    with open(file_path, "w", encoding="utf-8") as f:
                        json.dump(data, f, indent=4, ensure_ascii=False)
                    logger.info(f"  âœ“ Fixed: {file}")
                except Exception as e:
                    logger.warning(f"  âš ï¸ Failed to fix encoding for {file}: {e}")
    except Exception as e:
        logger.error(f"Error accessing output directory: {e}")


def run_batch_analysis(args):
    """
    ë°°ì¹˜ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        args: ArgumentParserì—ì„œ íŒŒì‹±ëœ ì¸ì
    """
    
    # .env íŒŒì¼ë¡œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    if os.path.exists(env_path):
        from dotenv import load_dotenv
        load_dotenv(dotenv_path=env_path)
        logger.info(f"âœ“ Loaded environment variables from: {env_path}")
        

    # LM ì„¤ì • ì´ˆê¸°í™”
    logger.info("Initializing LM configurations...")
    lm_configs = setup_lm_configs(args.model_provider)

    # ëª¨ë¸ëª… ê²°ì • (DB ì €ì¥ìš©)
    if args.model_provider == "gemini":
        current_model_name = "gemini"
    else:
        current_model_name = "openai"

    # HybridRM ì´ˆê¸°í™” (ë‚´ë¶€ DB + ì™¸ë¶€ ê²€ìƒ‰ í˜¼í•©)
    logger.info("Initializing HybridRM (Internal DB + External Search)...")
    
    # ë‚´ë¶€ ê²€ìƒ‰: PostgresRM (DART ë³´ê³ ì„œ)
    internal_rm = PostgresRM(k=args.search_top_k, min_score=args.min_score)
    logger.info(f"âœ“ Internal RM (PostgresRM) initialized with k={args.search_top_k}")
    
    # ì™¸ë¶€ ê²€ìƒ‰: SerperRM (Google Search)
    serper_api_key = os.getenv("SERPER_API_KEY")
    if not serper_api_key:
        logger.warning("âš ï¸ SERPER_API_KEY not found. External search will be disabled.")
        logger.warning("   Set SERPER_API_KEY to enable hybrid search.")
        return  # ì™¸ë¶€ ê²€ìƒ‰ í‚¤ ì—†ìœ¼ë©´ ë°°ì¹˜ ì¤‘ë‹¨
    else:
        external_rm = SerperRM(serper_search_api_key=serper_api_key, k=args.search_top_k)
        logger.info(f"âœ“ External RM (SerperRM) initialized with k={args.search_top_k}")
        
        # HybridRM ì¡°í•© (3:7 ë¹„ìœ¨)
        rm = HybridRM(internal_rm, external_rm, internal_k=3, external_k=7)
        logger.info("âœ“ HybridRM initialized with internal_k=3, external_k=7 (3:7 ratio)")

    # ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì§€ì •ëœ ì •ë³´ ì‚¬ìš©
    company_id = args.company_id
    company_name = args.company_name
    analysis_topic = args.analysis_topic  # UIì—ì„œ ì„ íƒëœ ë¶„ì„ ì£¼ì œ ì¹´í…Œê³ ë¦¬
    ai_query = f"{company_name} {analysis_topic}"  # LLMì—ê²Œ ì…ë ¥ë˜ëŠ” ì‹¤ì œ ì§ˆë¬¸
 

    logger.info("=" * 60)
    logger.info(f"Starting Enterprise STORM Batch Analysis")
    logger.info(f"Model provider: {args.model_provider} ({current_model_name})")
    logger.info(f"Total report titles to process: 1")
    logger.info(f"Output directory: {args.output_dir}")
    logger.info(f"Company: {company_name} (ID: {company_id})")
    logger.info("=" * 60)
    
    successful = True

    topic_start_time = datetime.now()
    logger.info("-" * 50)

    try:
        # ê¸°ì—… ì •ë³´ ê²€ì¦
        if not company_id or not company_name:
            logger.error("âŒ company_id and company_name are required")
            raise ValueError("Company ID and name are required")

        # ì‹¤í–‰ë³„ë¡œ ë³„ë„ í´ë” êµ¬ì„±: base/YYYYMMDD_HHMMSS_company_id_company_name/
        run_output_dir = build_run_output_dir(args.output_dir, company_id, company_name)
        logger.info(f"ğŸ“ Run output directory: {run_output_dir}")

        # Engine Arguments ì„¤ì • (output_dirì„ run_output_dirë¡œ ì§€ì •)
        engine_args = STORMWikiRunnerArguments(
            output_dir=run_output_dir,
            max_conv_turn=args.max_conv_turn,
            max_perspective=args.max_perspective,
            search_top_k=args.search_top_k,
            max_thread_num=args.max_thread_num,
        )

        # Runner ìƒì„±
        runner = STORMWikiRunner(engine_args, lm_configs, rm)

        # STORM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        runner.run(
            topic=ai_query,
            do_research=args.do_research,
            do_generate_outline=args.do_generate_outline,
            do_generate_article=args.do_generate_article,
            do_polish_article=args.do_polish_article,
        )
        runner.post_run()
        runner.summary()

        # ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ ì‹¤í–‰ ì„¤ì • ì €ì¥
        write_run_args_json(
            run_output_dir,
            topic=analysis_topic,
            company_id=company_id,
            company_name=company_name,
            args=args,
            model_name=current_model_name,
        )

        # DB ì €ì¥ ì „ì— 'ë°©ê¸ˆ ë§Œë“  í´ë”'ë§Œ ì¸ì½”ë”© ë³´ì • ìˆ˜í–‰
        fix_topic_json_encoding(ai_query, run_output_dir)

        # DBì— ê²°ê³¼ ì €ì¥
        save_report_to_db(ai_query, run_output_dir, "secrets_path", model_name=current_model_name, company_id=company_id, company_name=company_name, analysis_topic=analysis_topic)
        elapsed = datetime.now() - topic_start_time
        logger.info(f"âœ“ Completed '{ai_query}' in {elapsed.total_seconds():.1f}s")

    except Exception as e:
        elapsed = datetime.now() - topic_start_time
        logger.error(f"âœ— Failed '{ai_query}' after {elapsed.total_seconds():.1f}s")
        logger.error(f"  Error: {e}")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
        import traceback
        logger.error("  Full traceback:")
        logger.error(traceback.format_exc())
        
        successful = False

        if args.stop_on_error:
            logger.error("Stopping due to --stop-on-error flag")
            raise

    finally:
        # PostgresRM ì—°ê²° ì¢…ë£Œ
        rm.close()

    # ìµœì¢… ìš”ì•½
    logger.info("")
    logger.info("=" * 60)
    logger.info("Batch Analysis Complete!")
    if successful:
        logger.info(f"  Successful!")
    else:
        logger.info(f"  Failed...")
    logger.info(f"  Output directory: {args.output_dir}")
    logger.info("=" * 60)


def main():
    parser = ArgumentParser(
        description="Enterprise STORM - ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë„êµ¬"
    )

    # ì‹¤í–‰ ëª¨ë“œ
    parser.add_argument(
        "--batch",
        action="store_true",
        help="ë°°ì¹˜ ëª¨ë“œë¡œ ì‹¤í–‰ (ANALYSIS_TARGETS ë¦¬ìŠ¤íŠ¸ ì¼ê´„ ì²˜ë¦¬). ë¯¸ì§€ì • ì‹œ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ.",
    )

    # ì¶œë ¥ ì„¤ì •
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./results/enterprise",
        help="ê²°ê³¼ë¬¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./results/enterprise)",
    )

    # ëª¨ë¸ ê³µê¸‰ì ì„ íƒ
    parser.add_argument(
        "--model-provider",
        type=str,
        choices=["openai", "gemini"],
        default="openai",
        help="ì‚¬ìš©í•  LLM ê³µê¸‰ì ì„ íƒ (openai ë˜ëŠ” gemini, ê¸°ë³¸ê°’: openai)",
    )

    # PostgresRM ì„¤ì •
    parser.add_argument(
        "--search-top-k",
        type=int,
        default=10,
        help="ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ kê°œ (ê¸°ë³¸ê°’: 10)",
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=0.5,
        help="ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.5)",
    )

    # STORM ì—”ì§„ ì„¤ì •
    parser.add_argument(
        "--max-conv-turn",
        type=int,
        default=3,
        help="ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--max-perspective",
        type=int,
        default=3,
        help="ìµœëŒ€ ê´€ì  ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--max-thread-num",
        type=int,
        default=3,
        help="ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )

    # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì •
    parser.add_argument(
        "--do-research",
        action="store_true",
        default=True,
        help="ë¦¬ì„œì¹˜ ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-generate-outline",
        action="store_true",
        default=True,
        help="ì•„ì›ƒë¼ì¸ ìƒì„± ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-generate-article",
        action="store_true",
        default=True,
        help="ì•„í‹°í´ ìƒì„± ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-polish-article",
        action="store_true",
        default=True,
        help="ì•„í‹°í´ ë‹¤ë“¬ê¸° ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )

    # ì—ëŸ¬ ì²˜ë¦¬
    parser.add_argument(
        "--stop-on-error",
        action="store_true",
        help="ì—ëŸ¬ ë°œìƒ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨",
    )

    args = parser.parse_args()

    # action="store_true"ì™€ default=Trueê°€ í•¨ê»˜ ì‚¬ìš©ë˜ë©´ í•­ìƒ Trueê°€ ë˜ë¯€ë¡œ
    # ê¸°ë³¸ê°’ì´ Trueì¸ í”Œë˜ê·¸ë“¤ì€ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    if not any([args.do_research, args.do_generate_outline,
                args.do_generate_article, args.do_polish_article]):
        args.do_research = True
        args.do_generate_outline = True
        args.do_generate_article = True
        args.do_polish_article = True

    # CLIì—ì„œ ê¸°ì—…/ì£¼ì œ ì„ íƒ í›„ ë‹¨ê±´ ì‹¤í–‰
    args.company_id, args.company_name, args.analysis_topic = select_company_and_topic()
        
    # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
    run_batch_analysis(args)


if __name__ == "__main__":
    main()

