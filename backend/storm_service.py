"""
STORM Engine Wrapper Service for Backend API Integration

Task ID: FEAT-Core-001-EngineIntegration
Task ID: FIX-Core-002-SaveLogic & Encoding (Post-Processing Bridge)
Purpose: FastAPIì™€ knowledge_storm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—°ê²°í•˜ëŠ” Wrapper Service

Architecture:
    - scripts/run_storm.pyì˜ ë©”ì¸ ë¡œì§ì„ í•¨ìˆ˜ í˜•íƒœë¡œ ë³€í™˜
    - argparse ì˜ì¡´ì„± ì œê±° â†’ í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¡œ ì…ë ¥ ë°›ìŒ
    - BackgroundTasksì™€ ì—°ë™í•˜ì—¬ ë¹„ë™ê¸° ì‹¤í–‰ ê°€ëŠ¥
    - âœ… Post-Processing Bridge: íŒŒì¼ ì½ê¸° â†’ DB ì €ì¥

Key Fix:
    - STORMWikiRunner.run()ì€ íŒŒì¼ë§Œ ìƒì„± (DB ì €ì¥ ì•ˆ í•¨)
    - ìˆ˜ì •: runner.run() í›„ íŒŒì¼ì„ ì½ì–´ DBì— INSERT (RETURNING id)
    - í•œê¸€ ì¸ì½”ë”©: ëª¨ë“  íŒŒì¼ ì½ê¸°ì— encoding='utf-8' ëª…ì‹œ

Usage:
    from backend.storm_service import run_storm_pipeline
    
    background_tasks.add_task(
        run_storm_pipeline,
        job_id="job-123",
        company_name="ì‚¼ì„±ì „ì",
        topic="ê¸°ì—… ê°œìš”",
        jobs_dict=JOBS
    )

Author: Backend Development Team
Created: 2026-01-17
Updated: 2026-01-17 (Post-Processing Bridge Implementation)
"""

import os
import sys
import json
import glob
import time
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from knowledge_storm import (
    STORMWikiRunnerArguments,
    STORMWikiRunner,
    STORMWikiLMConfigs,
)
from knowledge_storm.lm import OpenAIModel, GoogleModel
from knowledge_storm.rm import PostgresRM
from knowledge_storm.utils import load_api_key

from src.common.config import extract_companies_from_query
from src.common.constants import (
    STORM_MAX_THREAD_LIMIT,
    STORM_DEFAULT_THREAD_COUNT,
    STORM_MAX_CONV_TURN,
    STORM_MAX_PERSPECTIVE,
    FILE_OPERATION_MAX_RETRIES,
    FILE_CHECK_WAIT_SECONDS,
    STORM_RUN_MAX_RETRIES,
    RATE_LIMIT_BASE_WAIT_SECONDS,
    PROGRESS_AFTER_RM_INIT,
    PROGRESS_STORM_RUNNING,
    PROGRESS_STORM_COMPLETED,
)
from src.common.logger import get_logger
from backend.database import get_db_cursor, get_db_connection
from psycopg2.extras import RealDictCursor, Json
import psycopg2
import psycopg2.extras

# âœ… [REFACTOR] Use centralized logger
logger = get_logger(__name__)


def _is_rate_limit_error(exc: Exception) -> bool:
    """
    Rate Limit ì—ëŸ¬ ë˜ëŠ” API ë¹ˆ ì‘ë‹µìœ¼ë¡œ ì¸í•œ IndexErrorë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    dspy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¹ˆ completions ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ìœ¼ë©´ IndexErrorê°€ ë°œìƒí•˜ëŠ”ë°,
    ì´ëŠ” ë³´í†µ Rate Limit(429)ë¡œ ì¸í•œ ë¹ˆ ì‘ë‹µ ë•Œë¬¸ì…ë‹ˆë‹¤.
    """
    msg = str(exc).lower()
    return (
        "rate limit" in msg
        or "429" in msg
        or "please try again" in msg
        or "list index out of range" in msg  # dspyê°€ ë¹ˆ ì‘ë‹µì„ ë°›ì„ ë•Œ
        or isinstance(exc, IndexError)       # IndexError íƒ€ì… ì§ì ‘ ê°ì§€
    )


# ============================================================
# Post-Processing Bridge Functions (FIX-Core-002)
# ============================================================
# ë¼ì´ë¸ŒëŸ¬ë¦¬(run())ëŠ” 'ì‘ê°€'ì¼ ë¿, ì›ê³ ë¥¼ ì„œê³ (DB)ì— ê½‚ëŠ” ê²ƒì€ 'ì‚¬ì„œ(Developer)'ê°€ ì§ì ‘ í•´ì•¼ í•©ë‹ˆë‹¤.

def _find_report_file(output_dir: str, max_retries: int = 10) -> str | None:
    """
    ì„ì‹œ í´ë”ì—ì„œ ìƒì„±ëœ ë¦¬í¬íŠ¸ íŒŒì¼ì„ **ê²°ì •ë¡ ì (Deterministic)**ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
    
    ì „ëµ: "ê²©ë¦¬ í›„ ì „ìˆ˜ ì¡°ì‚¬ (Isolate & Capture)"
    1. íŒŒì¼ëª…ì„ ì¶”ì¸¡í•˜ì§€ ì•ŠìŒ
    2. Globìœ¼ë¡œ .txt íŒ¨í„´ ì „ìˆ˜ ì¡°ì‚¬
    3. Retry ë¡œì§ìœ¼ë¡œ íŒŒì¼ ì‹œìŠ¤í…œ ì§€ì—° ëŒ€ì‘
    
    Args:
        output_dir: runnerê°€ ì‘ì—…í•œ ì„ì‹œ í´ë” (ì˜ˆ: ./results/temp/job-xyz)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 10ì´ˆ)
    
    Returns:
        íŒŒì¼ ê²½ë¡œ (ë¬¸ìì—´) ë˜ëŠ” None
    
    Example:
        file_path = _find_report_file("./results/temp/job-abc123")
        # â†’ "./results/temp/job-abc123/storm_gen_article_polished.txt"
    """
    if not os.path.exists(output_dir):
        logger.error(f"Output directory not found: {output_dir}")
        return None
    
    logger.info(f"Searching for report file in: {output_dir}")
    
    # ============================================================
    # Retry ë¡œì§: íŒŒì¼ ì‹œìŠ¤í…œ ì“°ê¸° ì§€ì—° ëŒ€ì‘ (ìµœëŒ€ 10ì´ˆ)
    # ============================================================
    target_file = None
    
    for attempt in range(max_retries):
        # 1. Globìœ¼ë¡œ ëª¨ë“  .txt íŒŒì¼ íƒìƒ‰ (recursive)
        all_txt_files = glob.glob(os.path.join(output_dir, "**/*.txt"), recursive=True)
        
        if not all_txt_files:
            logger.debug(f"  [{attempt+1}/{max_retries}] No .txt files found yet, waiting...")
            time.sleep(FILE_CHECK_WAIT_SECONDS)
            continue
        
        # 2. ìš°ì„ ìˆœìœ„: "article" ë˜ëŠ” "polished" í‚¤ì›Œë“œ í¬í•¨
        priority_keywords = ["polished", "article"]
        candidates = []
        
        for keyword in priority_keywords:
            matches = [f for f in all_txt_files if keyword in os.path.basename(f).lower()]
            if matches:
                candidates.extend(matches)
        
        # 3. í›„ë³´ê°€ ì—†ìœ¼ë©´ ê°€ì¥ í° íŒŒì¼ ì„ íƒ (ë³´í†µ ìµœì¢… ë¦¬í¬íŠ¸ê°€ ê°€ì¥ í¼)
        if not candidates:
            candidates = sorted(all_txt_files, key=lambda f: os.path.getsize(f), reverse=True)
        
        # 4. ì²« ë²ˆì§¸ í›„ë³´ ì„ íƒ
        if candidates:
            target_file = candidates[0]
            logger.info(f"âœ“ Found report file: {os.path.basename(target_file)} (attempt {attempt+1})")
            break
        
        time.sleep(FILE_CHECK_WAIT_SECONDS)
    
    # ============================================================
    # ë””ë²„ê¹…: íŒŒì¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° í´ë” ë‚´ìš© ì¶œë ¥
    # ============================================================
    if not target_file:
        try:
            all_files = os.listdir(output_dir)
            logger.error(f"âŒ Report file not found after {max_retries} retries")
            logger.error(f"   Directory: {output_dir}")
            logger.error(f"   Existing files: {all_files}")
        except Exception as e:
            logger.error(f"âŒ Failed to list directory: {e}")
        return None
    
    return target_file


def _read_report_content(file_path: str) -> str | None:
    """
    ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ íŒŒì¼ì„ UTF-8ë¡œ ì½ì–´ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
    
    âš ï¸ ì¤‘ìš”: encoding='utf-8' ëª…ì‹œì  ì„ ì–¸ìœ¼ë¡œ í•œê¸€ ì¸ì½”ë”© ê¹¨ì§ ë°©ì§€
    
    Args:
        file_path: ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        íŒŒì¼ ë‚´ìš© (ë¬¸ìì—´) ë˜ëŠ” None (ì½ê¸° ì‹¤íŒ¨ ì‹œ)
    
    Example:
        content = _read_report_content("./results/temp/job-abc123/storm_gen_article_polished.txt")
        # â†’ "# ì‚¼ì„±ì „ì ê¸°ì—… ê°œìš”\n\n## 1. ê°œìš”\n..."
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        logger.info(f"âœ“ Read report file ({len(content)} bytes)")
        return content
    except UnicodeDecodeError as e:
        logger.error(f"âŒ UTF-8 Encoding error: {e}")
        logger.warning("Retrying with fallback encoding (cp949)...")
        try:
            with open(file_path, "r", encoding="cp949") as f:
                content = f.read()
            logger.warning(f"âš ï¸  Fallback encoding used (cp949)")
            return content
        except Exception as e2:
            logger.error(f"âŒ Fallback encoding also failed: {e2}")
            return None
    except Exception as e:
        logger.error(f"âŒ Failed to read report file: {e}")
        return None


def _save_report_to_db(
    company_name: str,
    topic: str,
    report_content: str,
    toc_text: str | None = None,
    references_data: dict | None = None,
    conversation_log: dict | None = None,
    meta_info: dict | None = None,
    model_name: str = "gpt-4o"
) -> int | None:
    """
    ë¦¬í¬íŠ¸ë¥¼ DBì˜ Generated_Reports í…Œì´ë¸”ì— **ëª¨ë“  ì»¬ëŸ¼**ì„ í¬í•¨í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    
    âœ… RETURNING id êµ¬ë¬¸ìœ¼ë¡œ ì¦‰ì‹œ primary key íšë“
    âœ… company_id ìë™ ì¡°íšŒ (Companies í…Œì´ë¸”ì—ì„œ)
    âœ… JSONB ì»¬ëŸ¼ ì €ì¥ (references_data, conversation_log, meta_info)
    
    Args:
        company_name: ê¸°ì—…ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
        topic: ìˆœìˆ˜ ì£¼ì œ (ê¸°ì—…ëª… ì œê±°ë¨, ì˜ˆ: "ê¸°ì—… ê°œìš”")
        report_content: ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ë‚´ìš©
        toc_text: ëª©ì°¨(Table of Contents) í…ìŠ¤íŠ¸ (ì„ íƒ)
        references_data: ì°¸ì¡° ì •ë³´ ë”•ì…”ë„ˆë¦¬ (url_to_info.json)
        conversation_log: ëŒ€í™” ë¡œê·¸ ë”•ì…”ë„ˆë¦¬
        meta_info: ë©”íƒ€ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ì‹¤í–‰ ì„¤ì • ë“±)
        model_name: ì‚¬ìš©ëœ LLM ëª¨ë¸ëª… (ê¸°ë³¸ê°’: gpt-4o)
    
    Returns:
        ìƒì„±ëœ report_id (ì •ìˆ˜) ë˜ëŠ” None (ì €ì¥ ì‹¤íŒ¨ ì‹œ)
    
    Example:
        report_id = _save_report_to_db(
            company_name="ì‚¼ì„±ì „ì",
            topic="ê¸°ì—… ê°œìš”",
            report_content="# ì‚¼ì„±ì „ì ê¸°ì—… ê°œìš”\n...",
            toc_text="1. ê°œìš”\n2. ì‚¬ì—…ë‚´ìš©",
            references_data={...},
            model_name="gpt-4o"
        )
        # â†’ 42 (ìƒì„±ëœ ID)
    """
    try:
        conn = get_db_connection()
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # ============================================================
        # Step 1: company_id ì¡°íšŒ (Companies í…Œì´ë¸”ì—ì„œ)
        # ============================================================
        company_id = None
        try:
            cur.execute(
                'SELECT id FROM "Companies" WHERE company_name = %s',
                (company_name,)
            )
            result = cur.fetchone()
            if result:
                company_id = result['id']
                logger.info(f"âœ“ Found company_id: {company_id} for '{company_name}'")
            else:
                logger.warning(f"âš ï¸  Company '{company_name}' not found in Companies table")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to query company_id: {e}")
        
        # ============================================================
        # Step 2: INSERT with all columns + RETURNING id
        # ============================================================
        sql = """
            INSERT INTO "Generated_Reports" 
            (company_name, company_id, topic, report_content, toc_text, 
             references_data, conversation_log, meta_info, model_name, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
            RETURNING id
        """
        
        cur.execute(sql, (
            company_name,
            company_id,
            topic,
            report_content,
            toc_text,
            Json(references_data) if references_data else None,
            Json(conversation_log) if conversation_log else None,
            Json(meta_info) if meta_info else None,
            model_name
        ))
        
        result = cur.fetchone()
        report_id = result['id'] if result else None
        
        conn.commit()
        cur.close()
        conn.close()
        
        logger.info(f"âœ“ Saved to DB - Report ID: {report_id}")
        logger.info(f"  - company_id: {company_id}")
        logger.info(f"  - toc_text: {'Yes' if toc_text else 'No'}")
        logger.info(f"  - references_data: {'Yes' if references_data else 'No'}")
        logger.info(f"  - conversation_log: {'Yes' if conversation_log else 'No'}")
        logger.info(f"  - meta_info: {'Yes' if meta_info else 'No'}")
        
        return report_id
        
    except psycopg2.Error as e:
        logger.error(f"âŒ DB Error: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {e}")
        return None


def _load_and_save_report_bridge(
    output_dir: str,
    company_name: str,
    topic: str,
    jobs_dict: dict,
    job_id: str,
    model_name: str = "gpt-4o"
) -> int | None:
    """
    Post-Processing Bridge: íŒŒì¼ ì‹œìŠ¤í…œ â†’ DB
    
    ì´ í•¨ìˆ˜ëŠ” ë‹¤ìŒì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. íŒŒì¼ íƒìƒ‰ (File Discovery)
    2. íŒŒì¼ ì½ê¸° (Read to Memory) - UTF-8 ëª…ì‹œ
    3. DB INSERT (Save to DB) - RETURNING id
    4. ìƒíƒœ ë™ê¸°í™” (Update Status) - jobs_dict ì—…ë°ì´íŠ¸
    
    Args:
        output_dir: runnerê°€ ì‘ì—…í•œ ì„ì‹œ í´ë” ê²½ë¡œ
        company_name: ê¸°ì—…ëª…
        topic: ìˆœìˆ˜ ì£¼ì œ
        jobs_dict: ë©”ëª¨ë¦¬ ì‘ì—… ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        job_id: ì‘ì—… ID
        model_name: LLM ëª¨ë¸ëª…
    
    Returns:
        ì €ì¥ëœ report_id (ì •ìˆ˜) ë˜ëŠ” None
    """
    logger.info(f"[{job_id}] Starting Post-Processing Bridge...")
    
    # ============================================================
    # Step 1: File Discovery - ë¦¬í¬íŠ¸ íŒŒì¼ ì°¾ê¸°
    # ============================================================
    report_file = _find_report_file(output_dir)
    if not report_file:
        logger.error(f"âŒ Report file not found in {output_dir}")
        jobs_dict[job_id]["message"] = "ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return None
    
    # ============================================================
    # Step 2: Read to Memory - UTF-8ë¡œ íŒŒì¼ ì½ê¸°
    # ============================================================
    report_content = _read_report_content(report_file)
    if not report_content:
        logger.error(f"âŒ Failed to read report content")
        jobs_dict[job_id]["message"] = "ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return None
    
    # ============================================================
    # Step 2.5: Read Additional Files (TOC, References, Logs)
    # ============================================================
    # TOC (Table of Contents)
    toc_text = None
    toc_file = os.path.join(output_dir, "storm_gen_outline.txt")
    logger.info(f"Looking for TOC file: {toc_file}")
    if os.path.exists(toc_file):
        try:
            with open(toc_file, "r", encoding="utf-8") as f:
                toc_text = f.read()
            logger.info(f"âœ“ Read TOC file ({len(toc_text)} bytes)")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to read TOC: {e}")
    else:
        logger.warning(f"âš ï¸  TOC file not found: {toc_file}")
    
    # References Data (url_to_info.json)
    references_data = None
    ref_file = os.path.join(output_dir, "url_to_info.json")
    logger.info(f"Looking for references file: {ref_file}")
    if os.path.exists(ref_file):
        try:
            with open(ref_file, "r", encoding="utf-8") as f:
                references_data = json.load(f)
            logger.info(f"âœ“ Read references data ({len(references_data)} items)")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to read references: {e}")
    else:
        logger.warning(f"âš ï¸  References file not found: {ref_file}")
    
    # Conversation Log (conversation_log.json)
    conversation_log = None
    conv_file = os.path.join(output_dir, "conversation_log.json")
    logger.info(f"Looking for conversation log: {conv_file}")
    if os.path.exists(conv_file):
        try:
            with open(conv_file, "r", encoding="utf-8") as f:
                conversation_log = json.load(f)
            logger.info(f"âœ“ Read conversation log")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to read conversation log: {e}")
    else:
        logger.warning(f"âš ï¸  Conversation log not found: {conv_file}")
    
    # ë””ë²„ê¹…: í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ì¶œë ¥
    try:
        all_files = os.listdir(output_dir)
        logger.info(f"ğŸ“ Files in output_dir: {all_files}")
    except Exception as e:
        logger.warning(f"Failed to list directory: {e}")
    
    # Meta Info (run configuration)
    meta_info = {
        "output_dir": output_dir,
        "job_id": job_id,
        "timestamp": datetime.now().isoformat(),
        "model_name": model_name
    }
    
    # ============================================================
    # Step 3: Save to DB - INSERT with ALL columns + RETURNING id
    # ============================================================
    report_id = _save_report_to_db(
        company_name=company_name,
        topic=topic,
        report_content=report_content,
        toc_text=toc_text,
        references_data=references_data,
        conversation_log=conversation_log,
        meta_info=meta_info,
        model_name=model_name
    )
    
    if report_id is None:
        logger.error(f"âŒ Failed to save report to DB")
        jobs_dict[job_id]["message"] = "DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return None
    
    # ============================================================
    # Step 4: Update Status - ë©”ëª¨ë¦¬ ìƒíƒœ ë™ê¸°í™”
    # ============================================================
    jobs_dict[job_id]["report_id"] = report_id
    jobs_dict[job_id]["message"] = f"ë¦¬í¬íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (Report ID: {report_id})"
    
    logger.info(f"âœ… Bridge completed: report_id={report_id}")
    return report_id


def _setup_lm_configs(model_provider: str = "openai") -> STORMWikiLMConfigs:
    """
    LLM Configuration ì„¤ì •
    
    Args:
        model_provider: "openai" ë˜ëŠ” "gemini"
    
    Returns:
        STORMWikiLMConfigs: ì„¤ì •ëœ LM Config ê°ì²´
    """
    lm_configs = STORMWikiLMConfigs()

    if model_provider == "gemini":
        # Gemini ëª¨ë¸ ì„¤ì •
        gemini_kwargs = {
            "api_key": os.getenv("GOOGLE_API_KEY"),
            "temperature": 1.0,
            "top_p": 0.9,
        }

        gemini_flash_model = "gemini-2.0-flash-exp"
        gemini_pro_model = "gemini-2.0-flash"

        conv_simulator_lm = GoogleModel(
            model=gemini_flash_model, max_tokens=2048, **gemini_kwargs
        )
        question_asker_lm = GoogleModel(
            model=gemini_flash_model, max_tokens=2048, **gemini_kwargs
        )
        outline_gen_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=4096, **gemini_kwargs
        )
        article_gen_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=8192, **gemini_kwargs
        )
        article_polish_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=8192, **gemini_kwargs
        )

        logger.info(f"âœ“ Using Gemini models: {gemini_flash_model} (fast), {gemini_pro_model} (pro)")

    else:
        # OpenAI ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’)
        openai_kwargs = {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "temperature": 1.0,
            "top_p": 0.9,
        }

        gpt_35_model_name = "gpt-4o-mini"
        gpt_4_model_name = "gpt-4o"

        conv_simulator_lm = OpenAIModel(
            model=gpt_35_model_name, max_tokens=500, **openai_kwargs
        )
        question_asker_lm = OpenAIModel(
            model=gpt_35_model_name, max_tokens=500, **openai_kwargs
        )
        outline_gen_lm = OpenAIModel(
            model=gpt_4_model_name, max_tokens=400, **openai_kwargs
        )
        article_gen_lm = OpenAIModel(
            model=gpt_35_model_name, max_tokens=3000, **openai_kwargs  # 700 â†’ 3000, ì—¬ê¸°ì„œ 30k tpm í•œë„ë¥¼ 100% ì´ˆê³¼í•œë‹¤ê³  í•¨. miniëŠ” í•œë„ê°€ ë„‰ë„‰í•˜ë‹¤ê³  í•œë‹¤. (í•œê¸€ ìƒì„± ì¶©ë¶„)
        )
        article_polish_lm = OpenAIModel(
            model=gpt_4_model_name, max_tokens=4000, **openai_kwargs  # ëˆ„ë½ë˜ì—ˆë˜ í° ê°’ ì¶”ê°€
        )

        logger.info(f"âœ“ Using OpenAI models: {gpt_35_model_name} (fast), {gpt_4_model_name} (pro)")

    lm_configs.set_conv_simulator_lm(conv_simulator_lm)
    lm_configs.set_question_asker_lm(question_asker_lm)
    lm_configs.set_outline_gen_lm(outline_gen_lm)
    lm_configs.set_article_gen_lm(article_gen_lm)
    lm_configs.set_article_polish_lm(article_polish_lm)

    return lm_configs


def run_storm_pipeline(
    job_id: str,
    company_name: str,
    topic: str,
    jobs_dict: dict,
    model_provider: str = "openai"
):
    """
    STORM ì—”ì§„ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜ (Background Taskìš©)
    
    Args:
        job_id: ì‘ì—… ì¶”ì ìš© ê³ ìœ  ID (ì˜ˆ: "job-123")
        company_name: ê¸°ì—…ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
        topic: ìˆœìˆ˜ ì£¼ì œ (ê¸°ì—…ëª… ì œê±°ëœ ìƒíƒœ, ì˜ˆ: "ê¸°ì—… ê°œìš”")
        jobs_dict: ì‘ì—… ìƒíƒœ ì €ì¥ìš© In-memory Dictionary
        model_provider: LLM í”„ë¡œë°”ì´ë” ("openai" ë˜ëŠ” "gemini")
    
    Flow:
        1. Status Update â†’ processing
        2. STORM ì—”ì§„ ì„¤ì • ë° ì‹¤í–‰
        3. DBì— ê²°ê³¼ ì €ì¥ (STORMWikiRunnerê°€ ìë™ ì €ì¥)
        4. ìµœì‹  report_id ì¡°íšŒ
        5. Status Update â†’ completed
    
    Exception Handling:
        - ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ statusë¥¼ "failed"ë¡œ ë³€ê²½í•˜ê³  ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥
    """
    try:
        logger.info(f"[{job_id}] Starting STORM Pipeline")
        logger.info(f"  Company: {company_name}")
        logger.info(f"  Topic: {topic}")
        logger.info(f"  Model Provider: {model_provider}")

        # ============================================================
        # Step 1: Update Status â†’ Processing
        # ============================================================
        jobs_dict[job_id]["status"] = "processing"
        jobs_dict[job_id]["progress"] = 10
        
        # ============================================================
        # Step 2: Load API Keys (í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
        # ============================================================
        # secrets.tomlì´ ìˆìœ¼ë©´ ë¡œë“œ (ì„ íƒì‚¬í•­)
        secrets_path = os.path.join(os.path.dirname(__file__), "..", "secrets.toml")
        if os.path.exists(secrets_path):
            load_api_key(toml_file_path=secrets_path)
            logger.info(f"âœ“ Loaded secrets from: {secrets_path}")
        
        # ============================================================
        # Step 3: Topic ì „ì²˜ë¦¬ (ì¤‘ìš”!)
        # ============================================================
        # APIì—ì„œëŠ” ì´ë¯¸ clean_topicì„ ë°›ì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ì œê±°
        clean_topic = topic.replace(company_name, "").strip()
        clean_topic = " ".join(clean_topic.split())  # ë‹¤ì¤‘ ê³µë°± ì •ê·œí™”
        
        # LLMì—ëŠ” "{company_name} {topic}" í˜•ì‹ìœ¼ë¡œ ì „ë‹¬
        full_topic_for_llm = f"{company_name} {clean_topic}".strip()
        
        logger.info(f"  Clean Topic: {clean_topic}")
        logger.info(f"  Full Topic for LLM: {full_topic_for_llm}")
        
        # ============================================================
        # Step 4: LM Configurations ì´ˆê¸°í™”
        # ============================================================
        jobs_dict[job_id]["progress"] = 20
        logger.info("Initializing LM configurations...")
        lm_configs = _setup_lm_configs(model_provider)
        
        # ============================================================
        # Step 5: PostgresRM ì´ˆê¸°í™” (ë‚´ë¶€ DB ê²€ìƒ‰)
        # ============================================================
        jobs_dict[job_id]["progress"] = 30
        logger.info("Initializing PostgresRM...")
        
        # MVP ìµœì í™” ì„¤ì • (ì†ë„ ìš°ì„ )
        search_top_k = 10
        min_score = 0.5
        
        rm = PostgresRM(k=search_top_k, min_score=min_score)
        rm.set_company_filter(company_name)
        
        logger.info(f"âœ“ PostgresRM initialized with k={search_top_k}, company_filter={company_name}")
        
        # ============================================================
        # Step 6: STORM Engine Arguments ì„¤ì •
        # ============================================================
        jobs_dict[job_id]["progress"] = 40
        
        # ê²©ë¦¬ëœ ì„ì‹œ ì €ì¥ì†Œ (Clean Room) - ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        output_dir = os.path.abspath(os.path.join("results", "temp", job_id))
        os.makedirs(output_dir, exist_ok=True)
        
        logger.info(f"âœ“ Clean room created: {output_dir}")
        
        # ============================================================
        # ë™ì‹œì„± ì œì–´ (Concurrency Control)
        # OpenAI Tier 1 í•œë„(30k TPM) ë³´í˜¸ë¥¼ ìœ„í•´ ìµœëŒ€ ìŠ¤ë ˆë“œë¥¼ ì œí•œ
        # ============================================================
        max_thread_num_env = os.getenv("STORM_MAX_THREAD_NUM")
        default_threads = STORM_DEFAULT_THREAD_COUNT  # âœ… [REFACTOR] Use constant
        
        if max_thread_num_env:
            # í™˜ê²½ ë³€ìˆ˜ê°€ ìˆì–´ë„ STORM_MAX_THREAD_LIMITë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ (ì•ˆì „ì¥ì¹˜)
            max_thread_num = min(int(max_thread_num_env), STORM_MAX_THREAD_LIMIT)
        else:
            max_thread_num = default_threads
        
        logger.info(f"â„¹ï¸  Thread count set to: {max_thread_num} (Safe limit applied)")

        engine_args = STORMWikiRunnerArguments(
            output_dir=output_dir,
            max_conv_turn=STORM_MAX_CONV_TURN,         # âœ… [REFACTOR] Use constant
            max_perspective=STORM_MAX_PERSPECTIVE,       # âœ… [REFACTOR] Use constant
            search_top_k=search_top_k,
            max_thread_num=max_thread_num,
        )
        
        logger.info(f"âœ“ Engine arguments configured")
        
        # ============================================================
        # Step 7: STORM Runner ì‹¤í–‰ (Long-running process!)
        # ============================================================
        jobs_dict[job_id]["progress"] = PROGRESS_STORM_RUNNING  # âœ… [REFACTOR] Use constant
        logger.info("Starting STORM Runner...")
        
        runner = STORMWikiRunner(engine_args, lm_configs, rm)
        
        # ì‹¤ì œ ìƒì„± ì‹¤í–‰ (1~2ë¶„ ì†Œìš”) with simple rate-limit retry
        max_run_retries = STORM_RUN_MAX_RETRIES  # âœ… [REFACTOR] Use constant
        for attempt in range(max_run_retries):
            try:
                runner.run(
                    topic=full_topic_for_llm,
                    do_research=True,
                    do_generate_outline=True,
                    do_generate_article=True,
                    do_polish_article=True
                )
                break
            except Exception as run_err:
                is_rate = _is_rate_limit_error(run_err)
                if is_rate and attempt < max_run_retries - 1:
                    wait_s = RATE_LIMIT_BASE_WAIT_SECONDS * (attempt + 1)  # âœ… [REFACTOR] Use constant
                    logger.warning(
                        f"Rate limit detected; retrying in {wait_s}s (attempt {attempt+1}/{max_run_retries})"
                    )
                    time.sleep(wait_s)
                    continue
                # Re-raise for outer handler
                raise
        
        jobs_dict[job_id]["progress"] = 80
        logger.info("âœ“ STORM Runner completed successfully")
        
        # ============================================================
        # Step 8: Post-Processing Bridge (FIX-Core-003!)
        # ============================================================
        # âš ï¸ ì¤‘ìš”: post_run()ê³¼ summary() ì „ì— íŒŒì¼ì„ ë¨¼ì € ì½ì–´ì•¼ í•¨!
        # ì´ìœ : post_run()ì´ ì¶”ê°€ íŒŒì¼ ì‘ì—…ì„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸
        # âœ… íŒŒì¼ ì½ê¸° â†’ DB ì €ì¥ â†’ Report ID íšë“
        jobs_dict[job_id]["progress"] = 85
        logger.info("Starting Post-Processing Bridge...")
        
        report_id = _load_and_save_report_bridge(
            output_dir=output_dir,
            company_name=company_name,
            topic=clean_topic,
            jobs_dict=jobs_dict,
            job_id=job_id,
            model_name="gpt-4o"  # ì°¨í›„ íŒŒë¼ë¯¸í„°ë¡œ ë³€ê²½ ê°€ëŠ¥
        )
        
        if report_id is None:
            raise Exception("Post-Processing Bridge failed: Report ID is None")
        
        # Post-processing (ì„ íƒì  - ë¡œê·¸ ìƒì„± ë“±)
        try:
            runner.post_run()
            runner.summary()
        except Exception as e:
            logger.warning(f"Post-run processing warning: {e}")
        
        # ============================================================
        # Step 9: Update Status â†’ Completed
        # ============================================================
        jobs_dict[job_id]["status"] = "completed"
        jobs_dict[job_id]["report_id"] = report_id
        jobs_dict[job_id]["progress"] = 100
        jobs_dict[job_id]["message"] = f"ë¦¬í¬íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (Report ID: {report_id})"
        
        logger.info(f"[{job_id}] âœ… Pipeline completed successfully")
        logger.info(f"  Report ID: {report_id}")
        
    except Exception as e:
        # ============================================================
        # Error Handling
        # ============================================================
        logger.error(f"[{job_id}] âŒ Pipeline failed: {e}")
        logger.exception("Full traceback:")
        
        jobs_dict[job_id]["status"] = "failed"
        if _is_rate_limit_error(e):
            jobs_dict[job_id]["message"] = "LLM rate limitì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif isinstance(e, IndexError):
            jobs_dict[job_id]["message"] = "LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤ (ê°€ëŠ¥í•œ rate limit)."
        else:
            jobs_dict[job_id]["message"] = f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        jobs_dict[job_id]["progress"] = 0
        
        # RMì´ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ì—°ê²° ì¢…ë£Œ
        try:
            if 'rm' in locals():
                rm.close()
        except:
            pass


# ============================================================
# ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
# ============================================================
if __name__ == "__main__":
    print("STORM Service module loaded successfully")
